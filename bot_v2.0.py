# -*- coding: utf-8 -*-
import os
import logging
import pandas as pd
import requests
import zipfile
import io
import json
import asyncio
from datetime import datetime, timezone, timedelta
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from openai import OpenAI
from telegram import Update
from telegram.ext import Application, CommandHandler, ContextTypes
from bs4 import BeautifulSoup

# ----------------------------
# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# ----------------------------
TELEGRAM_BOT_TOKEN = ""
CLOUD_API_KEY = ''

DATA_FILE = "df_fin.csv"
META_FILE = "top_clusters_meta.csv"
LAST_UPDATE_FILE = "last_update.txt"
RADAR_CACHE_FILE = "radar_cache.json"
TRUSTED_SOURCES_FILE = "trusted_sources.json"

client = OpenAI(
    api_key=CLOUD_API_KEY,
    base_url="https://foundation-models.api.cloud.ru/v1"
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ----------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ----------------------------

def today_iso():
    return datetime.now(timezone.utc).strftime("%Y-%m-%d")

def was_updated_today():
    if not os.path.exists(LAST_UPDATE_FILE):
        return False
    with open(LAST_UPDATE_FILE, "r") as f:
        last = f.read().strip()
    if last != today_iso():
        return False
    if not os.path.exists(DATA_FILE) or not os.path.exists(META_FILE):
        return False
    return True

def mark_updated_today():
    with open(LAST_UPDATE_FILE, "w") as f:
        f.write(today_iso())
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö
    for f in [RADAR_CACHE_FILE, TRUSTED_SOURCES_FILE]:
        if os.path.exists(f):
            os.remove(f)

def extract_slug_words_from_url(url):
    match = re.search(r'/([^/]+?)(?:\.html?|\.php|\.asp|\.aspx|#|\?|/$|$)', url)
    if match:
        slug = match.group(1).lower()
        slug_clean = re.sub(r'\b\d+\b', '', slug)
        slug_clean = re.sub(r'[^\w-]', ' ', slug_clean)
        words = [w for w in re.split(r'[-\s]+', slug_clean) if w]
        if words:
            return ' '.join(words)
    return ''

def safe_get_text(url):
    try:
        response = requests.get(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (compatible; Bot/1.0)'},
            timeout=5
        )
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        article_body = (
            soup.find('article') or
            soup.find('div', class_='post-body') or
            soup.find('div', class_='content') or
            soup.find('body')
        )
        if article_body:
            return article_body.get_text(strip=True)
    except Exception as e:
        logger.warning(f"Failed to fetch {url}: {e}")
    return ""

def escape_html(text):
    return (
        text.replace("&", "&amp;")
            .replace("<", "<")
            .replace(">", ">")
    )

def fix_json(text):
    start = text.find("{")
    end = text.rfind("}") + 1
    if start != -1 and end != 0:
        return text[start:end]
    return text

def generate_full_analysis(headline, entities, sources, timeline_str, full_text_preview, hotness, cluster_id):
    sources_list = sources[:3]
    prompt = f"""
–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ù–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∏–∂–µ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

–¢—Ä–µ–±—É–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç (—Å—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π!):
{{
  "headline": "–ö—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–¥–æ 8 —Å–ª–æ–≤)",
  "hotness": {hotness},
  "why_now": "1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ –°–ï–ô–ß–ê–°? –£–ø–æ–º—è–Ω–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ—Å—Ç—å, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è, –º–∞—Å—à—Ç–∞–± —ç—Ñ—Ñ–µ–∫—Ç–∞.",
  "entities": ["—Å–ø–∏—Å–æ–∫", "–∫–ª—é—á–µ–≤—ã—Ö", "–∞–∫—Ç–∏–≤–æ–≤", "—Å—Ç—Ä–∞–Ω", "—Å–µ–∫—Ç–æ—Ä–æ–≤"],
  "sources": {json.dumps(sources_list)},
  "timeline": "{timeline_str}",
  "draft": {{
    "title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –∑–∞–º–µ—Ç–∫–∏",
    "lead": "–õ–∏–¥-–∞–±–∑–∞—Ü (1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
    "bullets": ["–ü—É–Ω–∫—Ç 1", "–ü—É–Ω–∫—Ç 2", "–ü—É–Ω–∫—Ç 3"],
    "quote": "–¶–∏—Ç–∞—Ç–∞ –∏–ª–∏ —Å–Ω–æ—Å–∫–∞ (–µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –Ω–∞–ø–∏—à–∏ ¬´–ù–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω–æ–π —Ü–∏—Ç–∞—Ç—ã¬ª)"
  }},
  "dedup_group": "{cluster_id}"
}}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
- –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ–±—ã—Ç–∏—è: {headline}
- –°—É—â–Ω–æ—Å—Ç–∏: {entities}
- –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources_list}
- –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è: {timeline_str}
- –§—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞: {full_text_preview[:600]}
"""
    response = client.chat.completions.create(
        model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        max_tokens=4000,
        temperature=0.3,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# ----------------------------
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è trusted_sources.json
# ----------------------------

def generate_trusted_sources():
    if not os.path.exists("unique_sources.txt"):
        logger.warning("–§–∞–π–ª unique_sources.txt –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫.")
        return {
            "reuters.com", "bloomberg.com", "ft.com", "wsj.com", "cnbc.com",
            "bbc.com", "economist.com", "ap.org", "nytimes.com", "rbc.ru",
            "interfax.ru", "tass.ru", "kommersant.ru", "abc.net.au", "9news.com.au"
        }

    with open("unique_sources.txt", "r", encoding="utf-8") as f:
        sources = [line.strip() for line in f if line.strip()][:200]

    prompt = f"""–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –º–µ–¥–∏–∞ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–µ.  
–û–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–∏–µ –∏–∑ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —è–≤–ª—è—é—Ç—Å—è **–Ω–∞–¥—ë–∂–Ω—ã–º–∏** –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.

–ö—Ä–∏—Ç–µ—Ä–∏–∏:
- –ò–∑–≤–µ—Å—Ç–Ω–æ–µ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–æ–µ –∏–ª–∏ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –°–ú–ò —Å —Ä–µ–ø—É—Ç–∞—Ü–∏–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
- –ù–µ –±–ª–æ–≥, –Ω–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –±–µ–∑ —Ä–µ–¥–∞–∫—Ü–∏–∏, –Ω–µ —Å–∞—Ç–∏—Ä–∞.

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON:
{{"trusted_sources": ["reuters.com", "bloomberg.com", ...]}}

–°–ø–∏—Å–æ–∫:
{chr(10).join('- ' + s for s in sources)}
"""
    try:
        response = client.chat.completions.create(
            model="deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
            max_tokens=2000,
            temperature=0.2,
            messages=[{"role": "user", "content": prompt}]
        )
        output = response.choices[0].message.content
        data = json.loads(fix_json(output))
        trusted = set(data.get("trusted_sources", []))
        with open(TRUSTED_SOURCES_FILE, "w", encoding="utf-8") as f:
            json.dump({"trusted_sources": list(trusted)}, f, indent=2, ensure_ascii=False)
        return trusted
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ trusted_sources: {e}")
        return {"reuters.com", "bloomberg.com", "ft.com", "wsj.com", "cnbc.com"}

# ----------------------------
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ç–æ–ø-–∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º hotness
# ----------------------------
def save_top_clusters_meta(df_fin, output_file=META_FILE):
    logger.info("üîç –ó–∞–ø—É—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã
    df_fin['V21DATE_dt'] = pd.to_datetime(df_fin['V21DATE'], format='%Y%m%d%H%M%S', errors='coerce')
    df_fin = df_fin.dropna(subset=['V21DATE_dt']).copy()

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ URL –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏
    df_fin['url_slug_words'] = df_fin['V2DOCUMENTIDENTIFIER'].apply(extract_slug_words_from_url)
    df_fin['semantic_text'] = (
        df_fin['V1THEMES'].fillna('') + ' ' +
        df_fin['V2ENHANCEDTHEMES'].fillna('') + ' ' +
        df_fin['V1PERSONS'].fillna('') + ' ' +
        df_fin['V2ENHANCEDPERSONS'].fillna('') + ' ' +
        df_fin['V1ORGANIZATIONS'].fillna('') + ' ' +
        df_fin['V2ENHANCEDORGANIZATIONS'].fillna('') + ' ' +
        df_fin['V1LOCATIONS'].fillna('') + ' ' +
        df_fin['V2ENHANCEDLOCATIONS'].fillna('') + ' ' +
        df_fin['V2EXTRASXML'].fillna('') + ' ' +
        df_fin['url_slug_words']
    )

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(df_fin['semantic_text'])
    clustering = DBSCAN(eps=0.01, min_samples=2, metric='cosine').fit(tfidf_matrix)
    df_fin['semantic_cluster'] = clustering.labels_

    # –ó–∞–≥—Ä—É–∂–∞–µ–º trusted sources
    if os.path.exists(TRUSTED_SOURCES_FILE):
        with open(TRUSTED_SOURCES_FILE, "r", encoding="utf-8") as f:
            trusted_sources = set(json.load(f)["trusted_sources"])
    else:
        trusted_sources = generate_trusted_sources()

    # –ë–µ—Ä—ë–º —Ç–æ–ø-10 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ —Ä–∞–∑–º–µ—Ä—É
    top_clusters = df_fin['semantic_cluster'].value_counts().head(10)
    meta_rows = []

    for cluster_id in top_clusters.index:
        if cluster_id == -1:
            continue

        cluster_data = df_fin[df_fin['semantic_cluster'] == cluster_id].copy()
        cluster_data = cluster_data.sort_values('V21DATE_dt')

        urls = cluster_data['V2DOCUMENTIDENTIFIER'].dropna().tolist()
        dates = cluster_data['V21DATE_dt'].dropna().tolist()
        sources = cluster_data['V2SOURCECOMMONNAME'].dropna().tolist()

        if not dates or not urls:
            continue

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        themes = cluster_data['V1THEMES'].dropna().str.split(';').explode().value_counts().head(10).index.tolist()
        persons = cluster_data['V1PERSONS'].dropna().str.split(';').explode().value_counts().head(5).index.tolist()
        orgs = cluster_data['V1ORGANIZATIONS'].dropna().str.split(';').explode().value_counts().head(5).index.tolist()

        start_date = min(dates)
        end_date = max(dates)

        # === –ù–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê –ì–û–†–Ø–ß–ï–°–¢–ò –ù–ê –û–°–ù–û–í–ï –í–°–ü–õ–ï–°–ö–ê ===
        cluster_data['hour'] = cluster_data['V21DATE_dt'].dt.floor('h')
        mentions_per_hour = cluster_data.groupby('hour').size()
        max_mentions_in_hour = mentions_per_hour.max() if not mentions_per_hour.empty else 0
        hotness = min(1.0, max_mentions_in_hour / 30.0)  # 30 —É–ø–æ–º–∏–Ω–∞–Ω–∏–π/—á–∞—Å = –º–∞–∫—Å–∏–º—É–º

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        num_sources = len(set(sources))

        meta_rows.append({
            "cluster_id": cluster_id,
            "urls": ";".join(urls[:10]),
            "themes": ";".join(themes[:5]),
            "persons": ";".join(persons[:5]),
            "orgs": ";".join(orgs[:5]),
            "start_date": start_date.isoformat(),
            "end_date": end_date.isoformat(),
            "hotness": round(hotness, 3),
            "num_sources": num_sources,
            "sources_list": sources
        })

    pd.DataFrame(meta_rows).to_csv(output_file, index=False)
    logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(meta_rows)} —Ç–æ–ø-–∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ {output_file}")

# ----------------------------
# –ó–∞–≥—Ä—É–∑–∫–∞ GDELT –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞
# ----------------------------

def fetch_gdelt_and_save():
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ GDELT –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞...")
    try:
        master_url = "http://data.gdeltproject.org/gdeltv2/masterfilelist.txt"
        response = requests.get(master_url, timeout=10)
        lines = response.text.strip().split('\n')
        now_utc = datetime.now(timezone.utc)
        cutoff_time = now_utc - timedelta(hours=24)
        gkg_urls_24h = []
        for line in reversed(lines):
            parts = line.strip().split()
            if len(parts) != 3:
                continue
            url = parts[2]
            if not url.endswith('.gkg.csv.zip'):
                continue
            filename = url.split('/')[-1]
            ts = filename.split('.')[0]
            try:
                file_time = datetime.strptime(ts, "%Y%m%d%H%M%S").replace(tzinfo=timezone.utc)
                if file_time >= cutoff_time:
                    gkg_urls_24h.append(url)
                else:
                    break
            except:
                continue
        all_dfs = []
        for url in gkg_urls_24h:
            try:
                resp = requests.get(url, timeout=10)
                with zipfile.ZipFile(io.BytesIO(resp.content)) as z:
                    fname = z.namelist()[0]
                    with z.open(fname) as f:
                        df = pd.read_csv(f, sep='\t', header=None, dtype=str, encoding='latin1')
                        all_dfs.append(df)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
                continue
        if not all_dfs:
            raise Exception("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞")
        df = pd.concat(all_dfs, ignore_index=True)
        gkg_columns = [
            "GKGRECORDID", "V21DATE", "V2SOURCECOLLECTIONIDENTIFIER", "V2SOURCECOMMONNAME",
            "V2DOCUMENTIDENTIFIER", "V1COUNTS", "V21COUNTS", "V1THEMES", "V2ENHANCEDTHEMES",
            "V1LOCATIONS", "V2ENHANCEDLOCATIONS", "V1PERSONS", "V2ENHANCEDPERSONS",
            "V1ORGANIZATIONS", "V2ENHANCEDORGANIZATIONS", "V15TONE", "V21ENHANCEDDATES",
            "V2GCAM", "V21SHARINGIMAGE", "V21RELATEDIMAGES", "V21SOCIALIMAGEEMBEDS",
            "V21SOCIALVIDEOEMBEDS", "V21QUOTATIONS", "V21ALLNAMES", "V21AMOUNTS",
            "V21TRANSLATIONINFO", "V2EXTRASXML"
        ]
        if len(df.columns) == len(gkg_columns):
            df.columns = gkg_columns
        all_themes = df['V1THEMES'].dropna()
        unique_themes = set()
        for theme_str in all_themes:
            themes = theme_str.split(';')
            for theme in themes:
                unique_themes.add(theme.strip())

        def starts_with_financial_theme(themes):
            if pd.isna(themes) or not isinstance(themes, str):
                return False
            first_theme = themes.split(';')[0].strip()
            return (
                first_theme.startswith('ECON_') or
                first_theme.startswith('TAX_FNCACT') or
                'FINANCE' in first_theme or
                first_theme.startswith('MARKET_') or
                first_theme.startswith('RATING_') or
                first_theme.startswith('COMMODITY_') or
                first_theme.startswith('CURRENCY_')
            )

        df['is_financial'] = df['V1THEMES'].apply(starts_with_financial_theme)
        
        df_fin = df[df['is_financial']].copy()
        if df_fin.empty:
            raise Exception("–ù–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        df_fin.to_csv(DATA_FILE, index=False)
        save_top_clusters_meta(df_fin)
        mark_updated_today()
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(df_fin)} –∑–∞–ø–∏—Å–µ–π, –∫–ª–∞—Å—Ç–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")
        return True
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ GDELT: {e}")
        return False

# ----------------------------
# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
# ----------------------------

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if was_updated_today():
        try:
            df_fin = pd.read_csv(DATA_FILE)
            df_fin['V21DATE_dt'] = pd.to_datetime(df_fin['V21DATE'], format='%Y%m%d%H%M%S', errors='coerce')
            min_date = df_fin['V21DATE_dt'].min()
            max_date = df_fin['V21DATE_dt'].max()
            count = len(df_fin)
            period = f"{min_date.strftime('%d.%m %H:%M')} ‚Äì {max_date.strftime('%d.%m %H:%M')}"
            await update.message.reply_text(
                f"‚úÖ –ë–∞–∑–∞ —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ —Å–µ–≥–æ–¥–Ω—è.\n"
                f"–°–æ–¥–µ—Ä–∂–∏—Ç {count:,} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–µ—Ä–∏–æ–¥ —Å {period}.\n"
                f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /radar."
            )
        except Exception as e:
            await update.message.reply_text("‚úÖ –ë–∞–∑–∞ —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ —Å–µ–≥–æ–¥–Ω—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /radar.")
        return
    await update.message.reply_text("üîÑ –ó–∞–ø—É—Å–∫–∞—é –∑–∞–≥—Ä—É–∑–∫—É –∏ –∞–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π... (~1‚Äì2 –º–∏–Ω)")
    success = fetch_gdelt_and_save()
    if success:
        try:
            df_fin = pd.read_csv(DATA_FILE)
            df_fin['V21DATE_dt'] = pd.to_datetime(df_fin['V21DATE'], format='%Y%m%d%H%M%S', errors='coerce')
            min_date = df_fin['V21DATE_dt'].min()
            max_date = df_fin['V21DATE_dt'].max()
            count = len(df_fin)
            period = f"{min_date.strftime('%d.%m %H:%M')} ‚Äì {max_date.strftime('%d.%m %H:%M')}"
            await update.message.reply_text(
                f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω!\n"
                f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count:,} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –ø–µ—Ä–∏–æ–¥ —Å {period}.\n"
                f"–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /radar –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞."
            )
        except:
            await update.message.reply_text("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /radar –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞.")
    else:
        await update.message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")

async def radar_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    # –û—Ç–ø—Ä–∞–≤–∫–∞ –∏–∑ –∫—ç—à–∞
    if os.path.exists(RADAR_CACHE_FILE):
        with open(RADAR_CACHE_FILE, "r", encoding="utf-8") as f:
            cached_data = json.load(f)
        await update.message.reply_text("üì¨ –û—Ç–ø—Ä–∞–≤–ª—è—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç...")
        await asyncio.sleep(5)
        for item in cached_data:
            output = (
                f"üî• <b>{item['headline']}</b>\n"
                f"üìä –ì–æ—Ä—è—á–µ—Å—Ç—å: {item['hotness']}\n"
                f"‚è± –ü–æ—á–µ–º—É —Å–µ–π—á–∞—Å: {item['why_now']}\n"
                f"üè∑ –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(item['entities'])}\n"
                f"üìÖ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è: {item['timeline']}\n"
                f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {' | '.join([f'<a href=\"{s}\">{i+1}</a>' for i, s in enumerate(item['sources']) if s])}\n\n"
                f"<b>–ß–µ—Ä–Ω–æ–≤–∏–∫:</b>\n"
                f"<b>{item['draft']['title']}</b>\n"
                f"{item['draft']['lead']}\n"
                f"‚Ä¢ {item['draft']['bullets'][0]}\n"
                f"‚Ä¢ {item['draft']['bullets'][1]}\n"
                f"‚Ä¢ {item['draft']['bullets'][2]}\n"
                f"<i>{item['draft']['quote']}</i>\n"
                f"üÜî –ö–ª–∞—Å—Ç–µ—Ä: {item['dedup_group']}"
            )
            await update.message.reply_text(output, parse_mode="HTML", disable_web_page_preview=True)
            await asyncio.sleep(1)
        await update.message.reply_text("‚úÖ –û—Ç—á—ë—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –∏–∑ –∫—ç—à–∞!")
        return

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    if not os.path.exists(META_FILE):
        await update.message.reply_text("‚ö†Ô∏è –ù–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –û—Ç–ø—Ä–∞–≤—å—Ç–µ /start.")
        return

    try:
        meta_df = pd.read_csv(META_FILE)
        if meta_df.empty:
            await update.message.reply_text("–ù–µ—Ç –∑–Ω–∞—á–∏–º—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å—É—Ç–∫–∏.")
            return
        meta_df = meta_df.sort_values('hotness', ascending=False).head(10)
        await update.message.reply_text(f"üì¨ –ì–µ–Ω–µ—Ä–∏—Ä—É—é —Ç–æ–ø-{len(meta_df)} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π...")

        cache_entries = []
        for _, top_row in meta_df.iterrows():
            urls = top_row['urls'].split(';')
            themes = top_row['themes'].split(';') if pd.notna(top_row['themes']) else []
            persons = top_row['persons'].split(';') if pd.notna(top_row['persons']) else []
            orgs = top_row['orgs'].split(';') if pd.notna(top_row['orgs']) else []
            entities = list(set(themes + persons + orgs))
            headline = themes[0] if themes else "–§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–µ"
            timeline_str = f"{top_row['start_date'][:16]} ‚Üí {top_row['end_date'][:16]}"
            full_text_preview = safe_get_text(urls[0]) or ""

            raw_output = generate_full_analysis(
                headline=headline,
                entities=entities,
                sources=urls,
                timeline_str=timeline_str,
                full_text_preview=full_text_preview,
                hotness=top_row['hotness'],
                cluster_id=top_row['cluster_id']
            )

            try:
                analysis = json.loads(fix_json(raw_output))
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON: {e}")
                continue

            cache_entries.append(analysis)

            output = (
                f"üî• <b>{analysis['headline']}</b>\n"
                f"üìä –ì–æ—Ä—è—á–µ—Å—Ç—å: {analysis['hotness']}\n"
                f"‚è± –ü–æ—á–µ–º—É —Å–µ–π—á–∞—Å: {analysis['why_now']}\n"
                f"üè∑ –°—É—â–Ω–æ—Å—Ç–∏: {', '.join(analysis['entities'])}\n"
                f"üìÖ –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è: {analysis['timeline']}\n"
                f"üîó –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {' | '.join([f'<a href=\"{s}\">{i+1}</a>' for i, s in enumerate(analysis['sources']) if s])}\n\n"
                f"<b>–ß–µ—Ä–Ω–æ–≤–∏–∫:</b>\n"
                f"<b>{analysis['draft']['title']}</b>\n"
                f"{analysis['draft']['lead']}\n"
                f"‚Ä¢ {analysis['draft']['bullets'][0]}\n"
                f"‚Ä¢ {analysis['draft']['bullets'][1]}\n"
                f"‚Ä¢ {analysis['draft']['bullets'][2]}\n"
                f"<i>{analysis['draft']['quote']}</i>\n"
                f"üÜî –ö–ª–∞—Å—Ç–µ—Ä: {analysis['dedup_group']}"
            )
            await update.message.reply_text(output, parse_mode="HTML", disable_web_page_preview=True)
            await asyncio.sleep(1)

        with open(RADAR_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache_entries, f, ensure_ascii=False, indent=2)

        await update.message.reply_text("‚úÖ –í—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à!")
    except Exception as e:
        logger.exception("–û—à–∏–±–∫–∞ –≤ /radar")
        await update.message.reply_text(f"‚ùå –û—à–∏–±–∫–∞: {str(e)[:200]}")

# ----------------------------
# –ó–∞–ø—É—Å–∫
# ----------------------------

def main():
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("radar", radar_command))
    logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –û—Ç–ø—Ä–∞–≤—å—Ç–µ /start –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö.")
    application.run_polling()

if __name__ == "__main__":
    main()
